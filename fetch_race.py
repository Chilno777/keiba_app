import time
import pandas as pd
import re
import json
import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup

# Seleniumé–¢ä¿‚
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager


# ==============================
# å‡ºé¦¬è¡¨ã®å–å¾—ï¼ˆSeleniumä½¿ç”¨ï¼‰
# ==============================
#horse_url
def fetch_syutubahyo_selenium(race_id):
    """Seleniumã§å‡ºé¦¬è¡¨ã‚’å–å¾—ï¼ˆURLæ­£è¦åŒ–ä»˜ãï¼‰"""
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from webdriver_manager.chrome import ChromeDriverManager
    from bs4 import BeautifulSoup
    import re
    from urllib.parse import urljoin
    import pandas as pd

    url = f"https://race.netkeiba.com/race/shutuba.html?race_id={race_id}"
    print(f"ğŸ“˜ ãƒ–ãƒ©ã‚¦ã‚¶ã§ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")

    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    driver.get(url)

    try:
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.RaceTable01"))
        )
    except:
        print("âš ï¸ å‡ºé¦¬è¡¨ãƒ­ãƒ¼ãƒ‰ã‚’å¾…æ©Ÿã—ã¾ã—ãŸãŒã€è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    rows = []
    table = soup.select_one("table.RaceTable01")
    if not table:
        print("âŒ å‡ºé¦¬è¡¨ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return pd.DataFrame()

    for tr in table.find_all("tr"):
        tds = tr.find_all("td")
        if len(tds) < 8:
            continue

        horse_a = tr.find("a", href=re.compile("/horse/"))
        if not horse_a:
            continue

        horse_name = horse_a.get_text(strip=True)
        raw_href = horse_a.get("href", "").strip()

        # --- URLæ­£è¦åŒ–å‡¦ç† ---
        if not raw_href:
            continue

        # db.netkeiba.com ã«çµ±ä¸€
        if raw_href.startswith("http"):
            horse_url = re.sub(r"^https*://race\.netkeiba\.com", "https://db.netkeiba.com", raw_href)
        else:
            horse_url = urljoin("https://db.netkeiba.com", raw_href)

        # äºŒé‡ã‚³ãƒ­ãƒ³ä¿®æ­£
        horse_url = horse_url.replace("https::", "https:")

        # é¨æ‰‹æƒ…å ±ãªã©ã‚’æŠ½å‡º
        jockey = tr.find("a", href=re.compile("/jockey/"))
        jockey_name = jockey.get_text(strip=True) if jockey else ""
        odds_tag = tr.find("td", class_="OddsTxt")
        odds = odds_tag.get_text(strip=True) if odds_tag else ""
        pop_tag = tr.find("td", class_="PopularTxt")
        pop = pop_tag.get_text(strip=True) if pop_tag else ""
        age = tds[3].get_text(strip=True)
        weight = tds[4].get_text(strip=True)

        rows.append({
            "é¦¬å": horse_name,
            "é¨æ‰‹": jockey_name,
            "æ–¤é‡": weight,
            "é¦¬é½¢": age,
            "äººæ°—": pop,
            "å˜å‹ã‚ªãƒƒã‚º": odds,
            "horse_url": horse_url
        })

    df = pd.DataFrame(rows)
    df.to_csv("data/syutubahyo_auto.csv", index=False, encoding="utf-8-sig")
    print(f"âœ… å‡ºé¦¬è¡¨ã‚’å–å¾—ã—ã¾ã—ãŸï¼ˆ{len(df)}é ­ï¼‰")
    return df


# ==============================
# å„é¦¬ã®éå»5èµ°ã‚’å–å¾—
# ==============================
import pandas as pd
import requests
from io import StringIO
from bs4 import BeautifulSoup
import re

def fetch_past_5races(horse_url, horse_name):
    """pandas.read_html() + BeautifulSoup ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½µç”¨ç‰ˆ"""
    print(f"ğŸ {horse_name} ã®è¿‘5èµ°ã‚’å–å¾—ä¸­...")
    try:
        res = requests.get(horse_url, headers={"User-Agent": "Mozilla/5.0"})
        res.encoding = "utf-8"
        html = res.text
    except Exception as e:
        print(f"âš ï¸ {horse_name}: ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•— ({e})")
        return []

    # --- æ–¹æ³•1: pandas.read_html() ---
    try:
        from io import StringIO
        dfs = pd.read_html(StringIO(html))
        for df in dfs:
            if any("ãƒ¬ãƒ¼ã‚¹" in str(c) for c in df.columns):
                df.columns = [re.sub(r'\s+', '', str(c)) for c in df.columns]
                race_col = [c for c in df.columns if "ãƒ¬ãƒ¼ã‚¹" in c][0]
                keep_cols = [race_col] + [c for c in ["æ—¥ä»˜", "ç€é †", "ã‚¿ã‚¤ãƒ ", "äººæ°—", "é€šé", "ç€å·®"] if c in df.columns]
                df = df[keep_cols].head(5)
                df["é¦¬å"] = horse_name
                print(f"âœ… {horse_name}: {len(df)}ä»¶å–å¾—ï¼ˆread_htmlï¼‰")
                return df.to_dict("records")
    except Exception:
        pass  # fallbackã¸

    # --- æ–¹æ³•2: BeautifulSoupã§divæ§‹é€ ã‚’è§£æ ---
    soup = BeautifulSoup(html, "html.parser")
    race_blocks = soup.select("div.HorseList, div.Horse_5result, table.Horse_5result_table")

    if not race_blocks:
        print(f"âš ï¸ {horse_name}: è¿‘èµ°ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ ({horse_url})")
        return []

    # æ‰‹å‹•ãƒ‘ãƒ¼ã‚¹ï¼ˆæœ€è¿‘ã®æ§‹æˆã«å¯¾å¿œï¼‰
    races = []
    rows = soup.select("tr.HorseList__row")
    for row in rows[:5]:
        cells = [td.get_text(strip=True) for td in row.find_all("td")]
        if len(cells) < 6:
            continue
        races.append({
            "é¦¬å": horse_name,
            "ãƒ¬ãƒ¼ã‚¹å": cells[1],
            "æ—¥ä»˜": cells[0],
            "ç€é †": cells[2],
            "ã‚¿ã‚¤ãƒ ": cells[3],
            "äººæ°—": cells[4],
            "ç€å·®": cells[5] if len(cells) > 5 else "",
        })

    print(f"âœ… {horse_name}: {len(races)}ä»¶å–å¾—ï¼ˆBeautifulSoup fallbackï¼‰")
    return races


# ==============================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==============================
def main():
    race_id = input("å–å¾—ã—ãŸã„ãƒ¬ãƒ¼ã‚¹IDã‚’å…¥åŠ›ï¼ˆä¾‹ï¼š202405040811ï¼‰: ").strip()
    df = fetch_syutubahyo_selenium(race_id)
    if df.empty:
        return

    all_races = []
    for _, row in df.iterrows():
        horse_name = row["é¦¬å"]
        horse_url = row["horse_url"]
        print(f"ğŸ {horse_name} ã®è¿‘5èµ°ã‚’å–å¾—ä¸­...")
        races = fetch_past_5races(horse_url, horse_name)
        all_races.extend(races)

    race_df = pd.DataFrame(all_races)
    race_df.to_csv("data/race_data_auto.csv", index=False, encoding="utf-8-sig")
    print("âœ… å…¨é¦¬ã®éå»5èµ°ã‚’ä¿å­˜ã—ã¾ã—ãŸ â†’ data/race_data_auto.csv")


if __name__ == "__main__":
    main()
