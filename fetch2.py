import json
import time
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

def fetch_syutubahyo_with_selenium(race_id):
    """Seleniumã§__NEXT_DATA__ã‚’å«ã‚€å‡ºé¦¬è¡¨ã‚’å–å¾—"""
    url = f"https://race.netkeiba.com/race/shutuba.html?race_id={race_id}"
    print(f"ğŸ“˜ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")

    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    driver.get(url)
    time.sleep(3)  # JSèª­ã¿è¾¼ã¿å¾…ã¡

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    script_tag = soup.find("script", id="__NEXT_DATA__")
    if not script_tag:
        print("âŒ __NEXT_DATA__ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚JavaScriptæœªå®Ÿè¡Œã®å¯èƒ½æ€§ã€‚")
        return pd.DataFrame()

    data = json.loads(script_tag.string)
    try:
        horses = data["props"]["pageProps"]["race"]["horses"]
    except KeyError:
        print("âš ï¸ horses ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ§‹é€ å¤‰æ›´ã®å¯èƒ½æ€§ã€‚")
        return pd.DataFrame()

    rows = []
    for h in horses:
        horse_name = h.get("name", "")
        jockey = h.get("jockey", {}).get("name", "")
        age = h.get("age", "")
        weight = h.get("burdenWeight", "")
        odds = h.get("odds", "")
        pop = h.get("popularity", "")
        horse_id = h.get("id", "")
        horse_url = f"https://db.netkeiba.com/horse/{horse_id}/"

        rows.append({
            "é¦¬å": horse_name,
            "é¨æ‰‹": jockey,
            "æ–¤é‡": weight,
            "é¦¬é½¢": age,
            "äººæ°—": pop,
            "å˜å‹ã‚ªãƒƒã‚º": odds,
            "horse_url": horse_url
        })

    df = pd.DataFrame(rows)
    df.to_csv("data/syutubahyo_auto.csv", index=False, encoding="utf-8-sig")
    print(f"âœ… å‡ºé¦¬è¡¨ã‚’å–å¾—ã—ã¾ã—ãŸï¼ˆ{len(df)}é ­ï¼‰")
    return df


def fetch_past_5races_nextdata(horse_url, horse_name):
    """é¦¬ã”ã¨ã®__NEXT_DATA__ã‹ã‚‰éå»5èµ°å–å¾—"""
    import requests
    res = requests.get(horse_url, headers={"User-Agent": "Mozilla/5.0"})
    res.encoding = "utf-8"
    soup = BeautifulSoup(res.text, "html.parser")

    script_tag = soup.find("script", id="__NEXT_DATA__")
    if not script_tag:
        print(f"âš ï¸ {horse_name}: __NEXT_DATA__ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ ({horse_url})")
        return []

    data = json.loads(script_tag.string)
    try:
        race_list = data["props"]["pageProps"]["horseResult"]["pastResults"]
    except KeyError:
        print(f"âš ï¸ {horse_name}: éå»èµ°ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return []

    races = []
    for r in race_list[:5]:
        races.append({
            "é¦¬å": horse_name,
            "ãƒ¬ãƒ¼ã‚¹å": r.get("raceName", ""),
            "ã‚°ãƒ¬ãƒ¼ãƒ‰": r.get("grade", ""),
            "æ—¥ä»˜": r.get("date", ""),
            "ã‚³ãƒ¼ã‚¹": r.get("courseName", ""),
            "ç€é †": r.get("finish", ""),
            "ã‚¿ã‚¤ãƒ ": r.get("time", ""),
            "äººæ°—": r.get("popularity", ""),
            "ç€å·®": r.get("margin", ""),
        })
    print(f"âœ… {horse_name}: {len(races)}ä»¶å–å¾—")
    return races


def main():
    race_id = input("å–å¾—ã—ãŸã„ãƒ¬ãƒ¼ã‚¹IDã‚’å…¥åŠ›ï¼ˆä¾‹ï¼š202405040811ï¼‰: ").strip()
    df = fetch_syutubahyo_with_selenium(race_id)
    if df.empty:
        return

    all_races = []
    for _, row in df.iterrows():
        horse_name = row["é¦¬å"]
        horse_url = row["horse_url"]
        print(f"ğŸ {horse_name} ã®è¿‘5èµ°ã‚’å–å¾—ä¸­...")
        races = fetch_past_5races_nextdata(horse_url, horse_name)
        all_races.extend(races)

    race_df = pd.DataFrame(all_races)
    race_df.to_csv("data/race_data_auto.csv", index=False, encoding="utf-8-sig")
    print("âœ… å…¨é¦¬ã®éå»5èµ°ã‚’ä¿å­˜ã—ã¾ã—ãŸ â†’ data/race_data_auto.csv")


if __name__ == "__main__":
    main()
